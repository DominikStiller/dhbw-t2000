Interleaving of the stages of multiple jobs is our key concept for enabling large-scale autotuning. \cite{Ma.2005} uses a design-time scheduler to create a program with good concurrency. We need to dynamically schedule incoming jobs, so the schedule cannot be predetermined. We need an additional component in the autotuning architecture that orchestrates running jobs. In this chapter, we describe the design and implementation of our central scheduler that controls stage execution.

\section{Design}
Our scheduler possesses the two features that have been determined to be imperative for optimal large-scale autotuning:
\begin{itemize}
	\item Computation resources are shared between jobs. This facilitates good resource utilization since the idle time of one job can be harnessed to execute another job. This is called \textit{interleaving} and saves hardware and costs as a result. However, stage dependencies of a single job must be maintained.
	\item Interference between jobs is prevented. This guarantees that inference performance and autotuning time are as good as possible. The scheduler needs to check if the resource that will be used by the next stage is free before execution. This might necessitate the postponing of stage executions if the stage is ready before the resource becomes free.
\end{itemize}
These two features not only make it match the optimal solution, but also do they solve the problem of bad resource utilization of single-job autotuning by leveraging that shortcoming.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{tvm_resource_utilization_interleaving}%
	\caption{Interleaving of multiple autotuning jobs}
	\label{fig:interleaving}
\end{figure}

Figure \ref{fig:interleaving} illustrates round-robin-based interleaving with an example of two jobs. Significant events are marked with numbers. Job A and Job B are started at the same time, but assume that the scheduler knows about A earlier. The first stage of A is executed, then the first stage of B. Once B finishes, the scheduler decides it is A's turn again and executes its second stage. Once A finishes the seconds stage, the client machine is free and B can execute the second stage. At the same time, A is ready to execute the third stage which will run on the target device. Since the target device is not in use, A can execute the profiling there in parallel to B's building since they use separate resources (\textbf{\textit{1}}). Building does not take as long as profiling, so A is ready to profile before B finishes its profiling stage. Therefore, A's third stage is postponed until the target device is free (\textbf{\textit{2}}). A's profiling and B's update model can, once again, execute simultaneously since they use distinct resources. After one iteration of all four stages, the process starts anew with the first stage (\textbf{\textit{3}}). This continues until both jobs are done. In a real scenario, new jobs might appear while other jobs are already running. The scheduler simply adds them to its list of jobs and includes them in the interleaving. Note how the resource utilization in Figure \ref{fig:interleaving} is much improved over the single-job autotuning in Figure \ref{fig:tvm-res-util} due to overlapping and sharing. Especially on the client device, utilization has almost been maximized since three of the four stages use the client.

\subsection{Autotuning Decomposition}
The default autotuning process is monolithic and can be regarded as a blackbox from the outside (Figure \ref{fig:autotuning-decomposition-before}). This means, the autotuning loop can be started, and it does not finish until the whole job is completed. Once a stage finishes, the next one is executed immediately. However, the scheduler needs to be able to control the execution of the individual stages because it needs to prevent interference by means of delayed execution. This necessitates the decomposition of the autotuning process into schedulable units, corresponding to the stages  (Figure \ref{fig:autotuning-decomposition-after}). The client does not execute any of the stages on its own. Rather, it provides an interface to execute schedulable units and waits for an external trigger to do so. The autotuning loop can now run in another component, such as the scheduler.

\begin{figure}
	\begin{minipage}[b]{.5\textwidth}
		\centering\includegraphics{autotuning_decomposition_before}
		\subcaption{Before decomposition}\label{fig:autotuning-decomposition-before}
	\end{minipage}%
	\begin{minipage}[b]{.5\textwidth}
		\centering\includegraphics{autotuning_decomposition_after}
		\subcaption{After decomposition}\label{fig:autotuning-decomposition-after}
	\end{minipage}
	\caption{Client interface before and after decomposition}
	\label{fig:autotuning-decomposition}
\end{figure}

\subsection{Scheduling Algorithm}
To fulfill the task of interleaving while preventing interference, the scheduler needs to know four pieces of information for every job:
\begin{itemize}
	\item Whether the current stage is done; lets scheduler know if the job is ready for next stage
	\item Whether the whole job is done; lets scheduler know if the job needs to be considered in the future
	\item Which resource the current stage runs on; lets scheduler know which resources are free and which are in use
	\item Which resource the next stage will run on; lets scheduler postpone stage execution to prevent interference
\end{itemize}
We call this \textit{load-awareness}; being aware of the state of jobs and resources as well as their relationships. Theoretically, this allows the scheduler to work not only with TVM's autotuning but any process that can provide this information. Designing our scheduler to be agnostic of the underlying process also simplifies the algorithm since no state of this process, i.e. the progress or position in the loop, needs to be considered.

In case multiple jobs are ready to execute a stage, the scheduler needs to decide which one to run. The simplest approach is to use a round-robin algorithm, which iterates over the jobs in the order they were started and picks the first one that is ready. More sophisticated approaches might apply some logic to decide on a job which would maximize resource utilization but keep the average autotuning time low. However, we choose the round-robin algorithm for our first version. It is easy to implement and works reasonably well for an arbitrary number of jobs, which allows us to proof our concept.

We present two algorithms that perform interleaving. The greedy algorithm lets a job execute the next stage directly after the previous one finishes, provided the resource is free. This might preempt the resource from another job who is already waiting to execute a stage on it. On the other hand, the fair scheduler features a queue for every resource, so the first job to be ready to run a stage on a resource will be the first one to actually use it. Each job is controlled using an interface that provides the four pieces of information necessary for scheduling (\pythoninline/is_stage_done()/, \pythoninline/is_complete()/, \pythoninline/previous_resource/, \pythoninline/next_resource/). The scheduler keeps a list of jobs (\pythoninline/jobs/) in the order in which they were registered. With each call of \texttt{next()}, the subsequent job is returned, wrapping around after the end is reached.  Furthermore, it keeps a list of resources which can be marked as free or busy (\pythoninline/mark_as_free()/, \pythoninline/mark_as_busy()/).
\begin{description}
	\item[Greedy interleaving] The algorithm for greedy interleaved scheduling is presented in Listing \ref{lst:sched-algo-interleaving-greedy}. The algorithm is an infinite loop, with each iteration operating on one job (Line 1). First, the next job in the job list is retrieved (Line 2). If there is no job, the algorithm tries again until one is registered (Lines 3--4). If the job is still busy executing a stage, it is not considered further in this iteration (Line 6). If the stage has finished, the resource it used is marked as free (Line 7). If the stage was the last stage in the job, the job can be removed from the list of jobs (Line 8--9). Otherwise, the job is ready to continue. If the next stage's resource is free, that resource is marked as busy and the stage is executed immediately (Lines 11-13).
\begin{listing}[h]
\begin{pythoncode}
while True:
    job = jobs.next()
    if not job:
        continue

    if job.is_stage_done():
        mark_as_free(job.previous_resource)
        if job.is_complete():
            jobs.remove(job)
            continue
        if is_free(job.next_resource):
           mark_as_busy(job.next_resource)
           job.next_stage.execute()
\end{pythoncode}
\unskip
\caption{Greedy interleaved scheduling pseudocode}
\label{lst:sched-algo-interleaving-greedy}
\end{listing}
	\item[Fair interleaving] The algorithm for fair interleaved scheduling is presented in Listing \ref{lst:sched-algo-interleaving-fair}. Each resource has a queue which contains stages that are ready and will use that resource (Line 1). The algorithm is an infinite loop (Line 2), with each iteration consisting of two phases: scheduling and execution. In the first phase, the scheduler checks for each job if the current stage is done (Lines 4--5). Busy jobs are not regarded further. If the stage is done, the resource that was used is marked as free (Line 6). If the job is complete, it can be removed from the list of jobs (Line 7--8). Otherwise, the job is ready to continue and the next stage is added to the queue of the resource that it will run on (Line 9--10). The iteration over all jobs in order is what effectively makes this round-robin scheduling. In the second phase, the scheduler iterates over each resource and the corresponding queue (Line 12). If the resource is free and there are pending stages for that resource, the first stage in the queue is dequeued and executed (Lines 13--15). The respective resource needs to be marked as busy (Line 16).
\begin{listing}[h]
\begin{pythoncode}
queues = {r: [] for r in resources}
while True:
 # Phase 1: Round-robin scheduling
 for job in jobs:
     if job.is_stage_done():
         mark_as_free(job.previous_resource)
         if job.is_complete():
             jobs.remove(job)
         elif not job.next_stage in queues[job.next_resource]:
             queues[job.next_resource].enqueue(job.next_stage)
 # Phase 2: Execution
 for resource, queue in queues:
     if is_free(resource) and len(queue) > 0:
         stage = queue.dequeue()
         stage.execute()
         mark_as_busy(resource)
\end{pythoncode}
\unskip
\caption{Fair interleaved scheduling pseudocode}
\label{lst:sched-algo-interleaving-fair}
\end{listing}
\end{description}

Additionally to interleaving, our scheduler supports two other strategies for executing multiple jobs, which will be used in the evaluation for comparison.

Sequential scheduling (Listing \ref{lst:sched-algo-sequential}) works similar to single-job autotuning without a scheduler. Jobs do not run in parallel, but the next job is only started when the previous one finishes. This renders consideration of resource free/busy state unnecessary, and stages do not need to be postponed. However, since it is controlled by the scheduler, we can calculate the overhead introduced by adding a scheduler component, e.g., due to communication between scheduler and client or scheduling itself.

\begin{listing}[t]
\begin{pythoncode}
while True:
    job = jobs.next()
    if not job:
        continue

    while not job.is_complete():
        if job.is_stage_done():
            job.next_stage.execute()
    jobs.remove(job)
\end{pythoncode}
\unskip
\caption{Sequential scheduling pseudocode}
\label{lst:sched-algo-sequential}
\end{listing}

Synchronous scheduling (Listing \ref{lst:sched-algo-synchronous}) forces parallel execution of the same stage of multiple jobs on the same resource, making it the exact opposite of interleaved scheduling. Postponed stage execution is applied here to guarantee full interference. We use this strategy to evaluate the worst case effect of interference. However, this only works for equal jobs, since there needs to be symmetry between stages of all jobs.

\begin{listing}[t]
\begin{pythoncode}
while True:
    current_jobs = jobs
    if len(current_jobs) < 2:
        continue

    while not any([j.is_complete() for j in current_jobs]):
        if all([j.is_stage_done() for j in current_jobs]):
            [j.next_stage.execute() for j in current_jobs]
    jobs.remove_all(current_jobs)
\end{pythoncode}
\unskip
\caption{Synchronous scheduling pseudocode}
\label{lst:sched-algo-synchronous}
\end{listing}

\subsection{Autotuning Process}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{tvm_autotuning_with_scheduler.pdf}%
	\caption{Autotuning process with scheduler}
	\label{fig:tvm-autotuning-with-scheduler}
\end{figure}

Figure \ref{fig:tvm-autotuning-with-scheduler} shows the autotuning process with our scheduler (compare with scheduler-less, single-job autotuning in Figure \ref{fig:tvm-autotuning}). There are two discrete control flows at work. The per-job autotuning control flow is the same as before, spanning client and target machines. The scheduling control flow is on a higher level incorporating multiple jobs, and spans scheduler and clients. A new job needs to be made known to the scheduler first so it can consider the job in scheduling. One client is responsible for running exactly one job, registering that job with the scheduler when launching. The client is then ready to receive control commands to execute individual stages. The client exposes the interface which is required by the scheduling algorithm while calling the respective TVM methods internally. Clients can share target devices since the scheduler prevents interference of the profiling stages of multiple jobs on the same resource. The normal autotuning process is then executed, however not in one monolithic step, but stage for stage, enabled by the decomposition. Possibly there are some waiting times between stages introduces by postponing.

Scheduler and clients live in different processes, usually even separate containers or physical servers. While the autotuning control flow already exists in form of in-process method calls and \gls{rpc} for remote profiling, the scheduling control flow requires its own \gls{rpc} infrastructure to enable communication between the scheduler and the clients.

In multi-client scenarios, the cost model of each client is initialized by transfer learning from the global autotuning database on the client machine, which contains data from all jobs that have previously been executed on that machine. During autotuning, measurement results are written into a job-specific database which is merged back into the global database when the job is complete.

\section{Implementation}
Since TVM provides the API for autotuning in Python only, we use Python 3.6 for our implementation. It is intended as a proof of concept which we want to develop rapidly to see if the interleaving scheduler delivers the expected results. Therefore, we create an implementation that does not offer much flexibility or fault tolerance. However, it is sufficient to perform experiments in our test environment. The scheduler implementation is built on top of SimpleTVM for interfacing with TVM's autotuning.

For the beginning, only a single client machine is supported, but an arbitrary number of clients can run on it. Multiple target devices can be utilized for profiling, but the scheduler regards all target devices as a single resource. This coarse granularity is another decision to facilitate simple implementation.

\subsection{Components}
The implementation of our scheduler is distributed over multiple components. Same-machine components interact via in-process method calls, but \gls{rpc} is required for cross-machine calls. We created a simple HTTP-based \gls{rpc} protocol to support communication between scheduler and clients, with both scheduler and client acting as HTTP server and client. However, the protocol is very specific to the required interface and not general-purpose. If requests fail, they are retried three times with exponential backoff.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{scheduler_layers}%
	\caption{Layers and components of scheduler implementation}
	\label{fig:scheduler-layers}
\end{figure}

The components act as layers of abstraction sitting on top of TVM to eventually hook into the actual autotuning process. Figure \ref{fig:scheduler-layers} shows the whole stack including method interfaces. We look closer at each component, from top to bottom.
\begin{description}
	\item[Scheduler] The \pythoninline/Scheduler/ implements the interleaved scheduling algorithms from Listings \ref{lst:sched-algo-interleaving-greedy} and  \ref{lst:sched-algo-interleaving-fair} as well as the sequential and synchronous strategies. The strategy can be specified when starting the scheduler. Instead of a list of jobs as in the algorithm, the scheduler actually keeps a list of the clients which run those jobs. The scheduler provides an \gls{rpc} interface for the client to register (not depicted in the figure) and will keep a reference to that client to enable controlling of its autotuning job. Additionally, some error handling is added to, e.g., remove clients if they become unreachable. Resource state is implemented in a resource--to--boolean dictionary, indicating if a given resource is free.
	\item[ClientStub] The \pythoninline/ClientStub/ does not perform any functionality on its own, rather it acts as an abstraction of the \pythoninline/Client/'s \gls{rpc} interface so the scheduler can call client methods as if they were an in-process object, allowing for a clean implementation of the scheduling algorithms. The \pythoninline/ClientStub/ passes through calls to the actual \pythoninline/Client/ but handles \gls{rpc} including serialization/deserialization of variables and error handling.
	\item[Client] The \pythoninline/Client/ provides the interface that is required by the scheduler to control jobs. It registers with the scheduler upon launch and exposes three methods via \gls{rpc}:
	\begin{itemize}
		\item \pythoninline/get_next_resource/ returns the resource that the next stage will run on, or \pythoninline/None/ if the job is complete, effectively combining two functionalities into one method. Furthermore, if the job is complete, the client shuts down after sending the \gls{rpc} response.
		\item \pythoninline/execute_next_stage/ calls the function for the next stage asynchronously and keeps a reference to the stage's thread in form of a future. If another stage is currently running, this methods fails.
		\item \pythoninline/is_stage_done/ returns a boolean, denoting if the stage that was previously executed using \pythoninline/execute_next_stage/ has finished. If no stage has been executed yet or the future indicates that the stage's thread has terminated,  \pythoninline/True/ is returned,  \pythoninline/False/ otherwise.
	\end{itemize}
	\item[JobManager] The \pythoninline/JobManager/ is responsible for negotiating between the simple resource- and stage-based interface required by the scheduler and the more complex interface of TVM's autotuning. This conversion from a stateless to a stateful interface requires that the \pythoninline/JobManager/ keep track of the current progress of the autotuning process, i.e. the position in the autotuning loop identified by task and stage. Effectively, it decides the order of stages in the loop, when the loop starts again, and when the loop terminates. This allows it to implement two methods: \pythoninline/get_next_resource/ is called directly by the client to get the resource that the next stage will run on, \pythoninline/get_next_stage/ returns a method containing the stage functionality which is executed by the client's \pythoninline/execute_next_stage/.\par
	The logic by which the next stage is decided in \pythoninline/get_next_stage/ is presented in Listing \ref{lst:stage-decision-algo}. \pythoninline/get_next_resource/ follows a similar logic but returns the resource instead of methods. The job's state is determined by flags indicating if the job has been initialized and finalized as well as the current task and stage in that task (Lines 1--2). If the job has not been initialized, the initialization method is returned and the \pythoninline/initialized/ flag is set (Lines 4--6). Then, the autotuning loop for the first task is started. With each call of \pythoninline/get_next_stage/, \pythoninline/current_stage/ is incremented and the respective stage's method is returned (Lines 8--22). The stage methods come from either the \pythoninline/Job/ or the decomposed TVM autotuning API. At the end of one loop iteration, which corresponds to one batch, the \pythoninline/current_stage/ is set to the first stage of the loop (Line 21). If the current batch was the last batch of the task (because either the search space has been exhausted or the specified number of trials has been reached), \pythoninline/current_task/ is incremented and \pythoninline/current_stage/ is reset to the task initialization so autotuning can commence for the next task (Lines 15--18). If all tasks have completed, the autotuning job is done (Line 7). Next comes the job finalization method (Lines 23--25), after which \pythoninline/None/ is always returned (Line 26).\par
\begin{listing}[h]
\begin{pythoncode}
initialized = finalized = False
current_task = current_stage = 0

if not initialized:
    initialized = True
    return initialize_job_fn
elif current_task < number_of_tasks:
    current_stage += 1
    if   current_stage == 1: return initialize_task_fn
    elif current_stage == 2: return select_batch_fn
    elif current_stage == 3: return build_fn
    elif current_stage == 4: return profile_fn
    elif current_stage == 5: return update_model_fn
    elif current_stage == 6:
        if last_batch:
            # Go to next task
            current_task += 1
            current_stage = 0
        else:
            # Go to select batch stage
            current_stage = 1
        return finish_batch_fn
elif not finalized:
    finalized = True
    return finalize_job_fn
else: return None
\end{pythoncode}
\unskip
\caption[{Pseudocode of JobManager's stage decision logic}]{Pseudocode of \pythoninline/JobManager/'s stage decision logic}
\label{lst:stage-decision-algo}
\end{listing}
	There is another version called \pythoninline/BundlingJobManager/ which exposes the same interface but instead of returning each stage individually, it bundles the stages for one resource into a single, larger stage. Thus, there are effectively only two stages, one for the client (select configurations, build, update model) and one for the target device (profiling), as opposed to \pythoninline/JobManager/ where each stage is individually schedulable. This might decrease resource idle time and speed up autotuning.
	\item[Job] The \pythoninline/Job/ is an abstract class which acts as interface specification for \pythoninline/TVMJob/ and \pythoninline/MockJob/. Jobs contain a collection of tasks as well as the initialization and finalization methods used by the \pythoninline/JobManager/.
	\item[TVMJob] The \pythoninline/TVMJob/ represents one autotuning job. It only passes calls through to \pythoninline/SimpleTVM/ which contains the implementations. The initialization method only stores a timestamp of the autotuning begin for measurement. The finalization method collects error and time statistics from the autotuning process and inserts them into the benchmarking context. Additionally, it merges the job-specific autotuning database with the global database. The collection of tasks is extracted using a method from TVM, but modified slightly by SimpleTVM.
	\item[MockJob] A \pythoninline/MockJob/ is a drop-in replacement for \pythoninline/TVMJob/, exposing the same interface but not running actual autotuning, which is a slow process. Rather, it simulates work by sleeping and prints to the console when stages are started and finished. This allows rapid debugging of the scheduler algorithms because no infrastructure like servers and the tracker need to be set up. The number of batches and tasks can be specified, as well as a time stretch factor to slow down or speed up the simulated work. The mock stages proportions are about the same as in real autotuning, e.g., profiling takes longer than building. It does not depend on any other components such as TVM.
\end{description}

\subsection{Usage}
Switching from scheduler-less default autotuning to scheduled autotuning is trivial if the former is already being used, as shown in Listing \ref{lst:simpletvm-flow-with-scheduler}. Creation of the \pythoninline/SimpleTVM/ object and import of the model is identical for both variants (Lines 1--2). Default autotuning is started by calling the appropriate method directly with the autotuning options such as number of trials or profiling timeout (Line 5). For scheduled autotuning, a \pythoninline/TVMJob/ needs to be created first (Line 8). A client for the job is then started with the host names of the client and scheduler machines as parameters (Line 9). After autotuning, the optimized module can be built, saved and evaluated as usual (compare Listing \ref{lst:simpletvm-flow}).
\begin{listing}[h]
\begin{pythoncode}
tvm = SimpleTVM(BenchmarkingContext('gpu'), rpc_tracker=('tracker', 9190))
tvm.from_model('resnet')

# Scheduler-less
tvm.autotune(options)

# Scheduled
job = TVMJob(tvm, options)
Client(job, client_host='client', scheduler_host='scheduler').start()
\end{pythoncode}
\unskip
\caption{Comparison of default and scheduled autotuning}
\label{lst:simpletvm-flow-with-scheduler}
\end{listing}

\subsection{Challenges}
For our very first prototype, we wanted all components to run in a single multi-threaded process, so we could evaluate our approach quickly without having to implement the \gls{rpc} protocol. However, Python does not support true multi-threading due to the global interpreter lock. This lock simplifies memory management for the interpreter but only one thread can execute code at a time because of it. Python's recommended replacement is launching other interpreter processes from within the code, however this requires serialization of objects for inter-process communication. Our nested class structures and passing around of methods that are created inside other methods caused problems with this serialization. That is why we had to implement \gls{rpc} before the actual scheduler to enable separate processes from the beginning.

Since multi-job autotuning with a scheduler requires setup of the infrastructure and is rather slow, we created the \pythoninline/MockJob/ class for improved debugging and testing of the scheduler functionality. This allows us to evaluate design choices in the scheduling algorithm much more rapidly. Moreover, the isolation from TVM narrows the room for errors which facilitates focusing on scheduler issues without being disrupted by problems caused by autotuning.

\section{Autotuning as a Service}
Setting up the multi-job autotuning infrastructure is cumbersome because a lot of components need to be configured and deployed, a lot of it imperatively in code. Additionally, provisioning of new client and target machines is not done automatically by the scheduler, which limits the scale at which autotuning can be performed. This motivates the need for an \gls{aaas} platform, which hides the complexity of building and maintaining the infrastructure and thus simplifies the application of autotuning, opening up the opportunities of autotuning for any developer of \gls{dl}-based software.

We imagine a solution that allows users to submit their trained model and a declarative specification, including information such as a target inference time to meet service-level agreements and the target device type such as specialized accelerators. The platform then automatically sets up the required machines and components, runs the autotuning process until the user's target inference time is achieved (or no further optimization is possible), then the optimized version is returned to the user. To this end, we propose a reference architecture. However, we do not implement the platform. \cite{Cho.2019} shows how the Kubernetes container management system could be harnessed for resource provisioning in an \gls{aaas} platform, with a prototype delivering promising results. Our architecture builds on top of such a system, however it does not prescribe any specific product.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{aaas_architecture}%
	\caption{Autotuning as a Service reference architecture}
	\label{fig:aaas-architecture}
\end{figure}

Figure \ref{fig:aaas-architecture} shows our proposed architecture. Users can submit their jobs including the model and specification through the portal. First the portal inserts that job information in the job database, after which it notifies the orchestrator about the new job. The orchestrator decides if the existing resources are sufficient, or if the client and target device cluster need to be scaled up, e.g., by booting new server instances on a cloud computing platform. The cluster controller, e.g., a Kubernetes Master, is responsible for executing the scaling. Then, the client is created on a client machine and the job is registered with the scheduler.
The scheduler now controls the autotuning jobs as described in the previous sections. However, the actual client functionality changes in the \gls{aaas} scenario. The cost model is extracted to a central server that all clients share, so neither a local cost model nor a job-specific autotuning database is necessary. The cost model server keeps a separate model for every target device. Furthermore, the client does not request profiling servers from a tracker, but the target devices are explicitly assigned per profiling stage by the scheduler. Profiling results are not returned to the client, but inserted into a global profiling database, which the cost model server uses to update the models periodically. Clients store their state in the job database as opposed to the JobManager object, which makes the clients stateless so they can resume the job in case of failure. This also allows the user to monitor autotuning progress through the portal. Once a job has completed, the resulting optimized TVM module is stored in the job database and the user can download it for deployment in his application.

Additionally, we make propose to increase autotuning speed. Firstly, before starting the autotuning, inference performance of a newly submitted model is checked with the existing best configurations. If the user's specifications are already met, no job needs to be launched. Secondly, jobs can be parallelized by splitting them into multiple jobs. Jobs consist of a set of tasks that are independent of each other. Moreover, the search space of each task can be divided to split one task into multiple. Leveraging task-level and search space-level splitting, autotuning can be accelerated if users request a higher number of resource be allocated to them. Furthermore, unused resources can be used better if the total platform load is low.

One limitation of \gls{aaas} over on-premise autotuning infrastructure is the set of supported target devices types. The platform can provide support for common accelerators, but if more exotic or novel hardware is desired, \gls{aaas} will fall short. One solution would be to support profiling on devices outside of the target device cluster. However, appropriate security measures would need to be taken.

In summary, the \gls{aaas} platform makes four important changes to the existing multi-job, scheduled autotuning architecture:
\begin{itemize}
	\item User-friendly portal instead of job specification in code
	\item Automatic resource provisioning/scaling instead of manual infrastructure setup
	\item Shared cost model between clients instead of job-local model
	\item External client state to recover from job failure instead of stateful clients
\end{itemize}
Instantiating such a platform is an important step towards making autotuning more accessible to enable more real-time \gls{dl} applications with little effort for developers.