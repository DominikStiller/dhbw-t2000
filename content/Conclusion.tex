Enabling large-scale autotuning is the foundation of any \gls{aaas} platform that uses our reference architecture. We demonstrated that our interleaving scheduler can leverage the fundamental weakness of resource idle time to make sharing of computation resources possible, which facilitates fast and economical autotuning while delivering fast inference performance. This is a key step towards democratizing real-time \gls{dl} applications to power exciting innovations in a wide variety of fields.

Future efforts based on our work should focus on creating a more intelligent scheduler algorithm to improve the efficiency of multi-job autotuning. A resource provisioning algorithm will then complete the orchestrator component, clearing the path for the development of a mature \gls{aaas} product.
