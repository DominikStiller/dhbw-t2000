AI is increasing in popularity
AI is used in many different areas
Users aren't experts
Existing products for easier setup and deployment of training and inference infrastructure by offering AI infrastructure as a service

\section{Problem}
Common applications like industrial monitoring or autonomous driving require real-time performance
accelerator hardware with device-specific model optimizations needed
Currently manual optimization
Requires deep knowledge -> not easy for non-expert users

required: automated inference performance optimization (autotuning)
To offer it as a service so it can be used by a larger audience requires it to be scalable
Current autotuning does not scale well
To the best of our knowledge, there is no existing solution.

\section{Scope}
In this paper, we design and develop the prototype of a central, load-aware scheduler to solve this problem
This scheduler controls multiple jobs that share computation resources to enable large-scale artificial neural network autotuning
First step, develop framework to examine capabilities and limitations of autotuning in different configurations on multiple accelerator devices
Allows us to find properties which we can leverage to parallelize autotuning
Design and create a working proof-of-concept implementation
Evaluate our scheduler design and compare with default implementation
Propose an Autotuning as a Service architecture as base for future work

Thesis:
Controlling the execution of multiple jobs by a load-aware scheduler makes large-scale autotuning more efficient in terms of
- autotuning completion time
- resulting inference performance and 
- hardware requirements

dont improve autotuning process itself, but propositions are made in future work
dont develop actual autotuning as a service product, but propose an architecture

Project was conducted by Hewlett Packard Labs
