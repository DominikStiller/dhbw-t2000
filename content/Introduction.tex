In recent years, \gls{ai} has garnered tremendous success, revolutionizing the way we work and accelerating economic growth  of industries and whole nation~\cite[p.~15~ff.]{Statista.2019}. Especially \gls{dl}, a subfield of \gls{ai}, has made vast improvements and is the prime method of modern \gls{ai}. In the future, \gls{ai} will be applied to even more areas, where non-expert users want to benefit from \gls{ai} without the technical complexity introduced by development and deployment of \gls{ai} applications. This is facilitated by platforms such as BlueData and Qubole, which automate infrastructure setup and provide user-friendly interfaces to make \gls{ai} and \gls{dl} more accessible. 

\section{Problem}
More and more applications like industrial monitoring or autonomous driving require real-time performance, most of them powered by \gls{dl}. Specialized accelerator hardware such as \glspl{gpu} or FPGAs are employed to speed up the computation-intensive inference. However, the model itself needs device-specific, low-level optimizations to harness the accelerator's full potential. Currently, these optimizations are manually developed by the device vendors who have deep knowledge of their hardware. \gls{dl} researchers who want to experiment with new model types and high-level optimizations are forced to wait until low-level implementations are supported by vendor libraries.

Automated performance optimization, called autotuning, creates optimized low-level implementations without the need for human experts in a vendor-agnostic way. This fosters innovation and helps manage the increasing performance demands for a growing variety of models and accelerator devices. While autotuning is already employed, it has yet to reach widespread apply, partially because it is still inconvenient to use. Offering autotuning as a service can make it accessible to a larger audience to facilitate real-time \gls{dl} applications, but requires support for large-scale autotuning. However, inefficiencies in the autotuning process prohibit efficient scaling and, in turn, the implementation of an Autotuning as a Service platform. To the best of our knowledge, there is no existing solution for scaling up autotuning.

\section{Scope}
In this thesis, we design and create a prototypical implementation of a load-aware scheduler to enable large-scale autotuning. This scheduler controls multiple autotuning jobs that share computation resources to overcome the inefficiencies of current autotuning. We show that controlling the execution of multiple jobs by a load-aware scheduler makes large-scale autotuning more efficient in terms of
\begin{itemize}
	\item autotuning completion time,
	\item resulting inference performance and
	\item hardware requirements.
\end{itemize}

First, we discuss manual and automated performance optimization before comparing two frameworks for autotuning (Chapter \ref{sec:background}). Next, we will develop a framework to examine the capabilities and limitations of autotuning in different scenarios. This will allows us to find the concept of interleaving which we can leverage to scale autotuning (Chapter \ref{sec:using-tvm}). We will design and implement our scheduler which is used in our proposed reference architecture for Autotuning as a Service (Chapter \ref{sec:autotuning-scheduler}). Finally, we evaluate our scheduler design and point to future improvements (Chapter \ref{sec:evaluation}). Our experiments show good results for resulting inference performance and hardware requirements.

No improvements are made to the autotuning process itself, but we base our work on the TVM~\cite{Chen.2018b} autotuning framework and enhance it with a further component. Also, we do not implement Autotuning as a Service. This thesis describes only a reference architecture, a prototype implementation is described in~\cite{Cho.2019}.

This project was conducted by the \textit{Networking, IoT and Mobility Laboratory} of the \textit{Hewlett Packard Labs}.
