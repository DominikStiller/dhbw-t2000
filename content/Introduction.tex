AI is increasing in popularity
AI is used in many different areas
Users aren't experts
Existing products for easier setup and deployment of training and inference infrastructure

\section{Problem}
Often real-time necessary (examples)
Look closer at Seagate
requires low inference time, every saved ms makes big difference when a lot of interences are being made
use of specific accelerator devices
Generic models perform poorly because they dont make full use of accelerator capabilities
Model needs to be attuned to accelerator
Requires deep knowledge -> not easy for non-expert users

\section{Motivation}
required: automated inference performance optimization (autotuning) so model performance can keep up with application demands
imagine autotuning as a service where users can submit their trained model and receive an optimized version according to SLA
Would make autotuning available for a wide audience on a large scale
Only autotuning implementation in deep learning compiler stack TVM (find evidence), what we will be using throughout this project
Only supports single jobs

\section{Scope}
In this paper, we enable large scale autotuning by developing a scheduler to orchestrate multiple jobs
Design and create a working proof-of-concept implementation

Research question:
What improvements of efficiency in terms of
- autotuning speed
- resulting inference performance and 
- resource utilization
does a scheduler have, compared to the default design?

dont improve autotuning process itself, but propositions are made in future work
dont develop actual autotuning as a service product, but propose an architecture